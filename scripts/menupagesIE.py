#This is query API endpoint: "https://extraction.import.io/query/extractor/{Extractor GUID}?_apikey={yourAPIKey}"

# This is the framework for chaining the Extractors required to do the menupages extraction
# You will need to parse the json at each step that you wish to collect data.

# Steps:
# 1. Get all links generated by Extractor1
# 2. Pass links from Extractor1 to Extractor2 and gather all links generated by Extractor2
# 3. Generate pagination links based on output from Extractor2 (links and number of restaurants per town) 

Extractor1 = "543a5f4c-c37d-49d6-a746-271a2e2d9a5b"
Extractor2 = "4ef4c7ff-671b-4b95-ad88-fc0faa5f797b"

import json
import urllib
import re
import math
import csv
import time # if you want to pause between queries

#It is best to store your API Key in a separate file - this is something that you should not share. Create yourself a .txt file and put your APIKey in this file.
apikey = open("apikey.txt")
apikey = apikey.read()
link_list = list() # here will be stored links from step 1 for passing to step 2
e = open('menupages_errors.txt','w') #file for errors
with open('menupages_urls.csv','wb') as f: # if you want to upload URLs directly to the import.io dashboard you can write the results to a .csv file and upload it manually
    writer = csv.writer(f)

    # 1. Get all links generated by Extractor1
    try:
        jsonurl = "https://extraction.import.io/query/extractor/"+Extractor1+"?_apikey="+apikey+"&url=http%3A%2F%2Fwww.menupages.co.uk"
        data = urllib.urlopen(jsonurl).read()
        info = json.loads(data)
        result = info['extractorData']['data'][0]['group']
        #print result
        for idx in range(0, len(result)):
            link = info['extractorData']['data'][0]['group'][idx]['link'][0]['href']
            link_list.append(link) 
    
    # handle errors
    except Exception as errors:
        e.write(str(errors.message) + '\n')
        pass

    # 2. Pass links from Extractor1 to Extractor2 and gather all links generated by Extractor2
    for line in link_list:
        try:
            jsonurl = "https://extraction.import.io/query/extractor/"+Extractor2+"?_apikey="+apikey+"&url=" + line
            data = urllib.urlopen(jsonurl).read()
            info = json.loads(data)
            result = info['extractorData']['data']

            for idx in range(0, len(result)):
                for idy in range(0, len(result[idx]['group'])):
                    # pagination links are generated based on number of restaurants per each area
                    link = result[idx]['group'][idy]['link'][0]['text']
                    number = int(result[idx]['group'][idy]['number'][0]['text'])


                    htmlfile = urllib.urlopen(link)
                    htmltext = htmlfile.read()

                    # 3. Generate pagination links based on output from Extractor2 (links and number of restaurants per town)
                    try:
                        match_offset = re.search(r"btnNext\" href=\"(.*\/)\d+\">", htmltext) #pagination appers when there is "next" button which contains url with offset (step 15) 
                        pagination = match_offset.group(1)
                        for k in range(0, number/15 + 1):
                            final_url = pagination + str(k*15)
                            print final_url
                            writer.writerow([final_url])


                    except:
                        final_url = link
                        print final_url
                        writer.writerow([final_url])

        # handle errors
        except Exception as errors:
            e.write(str(errors.message) + '\n')
            pass


f.close()
#now you can open menupages_urls.csv with generated urls and use all these links for the next extractor in dash - JustEat 3- menupages (linksToRest) cc285cb7-4709-46f4-a743-383f4fb73547

        
         

